{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('test.csv', header=None, names=['haiku'], delimiter=\"@\")\n",
    "df = pd.read_parquet('dataset2.parquet')\n",
    "df = df.rename(columns={\"text\":\"haiku\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>haiku</th>\n",
       "      <th>text_phonemes</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keyword_phonemes</th>\n",
       "      <th>gruen_score</th>\n",
       "      <th>text_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>What did you order? / I forgot what I wanted. ...</td>\n",
       "      <td>waht dihd yuw aor|der / ay fer|gaat waht ay wa...</td>\n",
       "      <td>you order</td>\n",
       "      <td>yuw aor|der</td>\n",
       "      <td>0.898392</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>My lover is gone. / Won't you come back to my ...</td>\n",
       "      <td>may lah|ver axz gaon / wownt yuw kahm baek tax...</td>\n",
       "      <td>come back</td>\n",
       "      <td>kahm baek</td>\n",
       "      <td>0.896483</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9866</th>\n",
       "      <td>twaiku</td>\n",
       "      <td>I just remembered. / I'm working tomorrow nigh...</td>\n",
       "      <td>ay jhahst rax|mehm|berd / ihm wer|kaxng tax|ma...</td>\n",
       "      <td>working tomorrow</td>\n",
       "      <td>wer|kihng tax|maa|row</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18866</th>\n",
       "      <td>twaiku</td>\n",
       "      <td>Your eyes deceive you. / An illusion fools you...</td>\n",
       "      <td>yaor ayz dax|siyv yuw / axn ax|luw|zhaxn fuwlz...</td>\n",
       "      <td>illusion fools</td>\n",
       "      <td>ax|luw|zhaxn fuwlz</td>\n",
       "      <td>0.893575</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Worker bees can leave. / Even drones can fly a...</td>\n",
       "      <td>wer|ker biyz kaen liyv / iy|vaxn drownz kaen f...</td>\n",
       "      <td>worker bees</td>\n",
       "      <td>wer|ker biyz</td>\n",
       "      <td>0.893551</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48902</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>Moonrise. / An owl swoops up. / Something.</td>\n",
       "      <td>muwn|rayz axn awl swuwps ahp sahm|thaxng</td>\n",
       "      <td>owl</td>\n",
       "      <td>awl</td>\n",
       "      <td>0.893530</td>\n",
       "      <td>Moonrise. An owl swoops up. Something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35750</th>\n",
       "      <td>haiku_data_1</td>\n",
       "      <td>Moonrise. / An owl swoops up. / Something.</td>\n",
       "      <td>muwn|rayz axn awl swuwps ahp sahm|thaxng</td>\n",
       "      <td>owl</td>\n",
       "      <td>awl</td>\n",
       "      <td>0.893530</td>\n",
       "      <td>Moonrise. An owl swoops up. Something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Change begins right now. / Let music and dance...</td>\n",
       "      <td>cheynjh bax|gihnz rayt naw / leht myuw|zaxk ae...</td>\n",
       "      <td>dance commence</td>\n",
       "      <td>daens kax|mehns</td>\n",
       "      <td>0.893291</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Winter is coming. / Cold winds are blowing ove...</td>\n",
       "      <td>wihn|ter axz kah|maxng / kowld wihndz aar blow...</td>\n",
       "      <td>cold winds</td>\n",
       "      <td>kowld wihndz</td>\n",
       "      <td>0.891717</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Your coldness burns me. / You stab me with ici...</td>\n",
       "      <td>yaor kowld|naxs bernz miy / yuw staeb miy wihd...</td>\n",
       "      <td>icicles</td>\n",
       "      <td>ay|sax|kaxlz</td>\n",
       "      <td>0.891561</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             source                                              haiku  \\\n",
       "5280        bfbarry  What did you order? / I forgot what I wanted. ...   \n",
       "1216        bfbarry  My lover is gone. / Won't you come back to my ...   \n",
       "9866         twaiku  I just remembered. / I'm working tomorrow nigh...   \n",
       "18866        twaiku  Your eyes deceive you. / An illusion fools you...   \n",
       "717         bfbarry  Worker bees can leave. / Even drones can fly a...   \n",
       "48902  haiku_data_2         Moonrise. / An owl swoops up. / Something.   \n",
       "35750  haiku_data_1         Moonrise. / An owl swoops up. / Something.   \n",
       "3741        bfbarry  Change begins right now. / Let music and dance...   \n",
       "978         bfbarry  Winter is coming. / Cold winds are blowing ove...   \n",
       "2056        bfbarry  Your coldness burns me. / You stab me with ici...   \n",
       "\n",
       "                                           text_phonemes          keywords  \\\n",
       "5280   waht dihd yuw aor|der / ay fer|gaat waht ay wa...         you order   \n",
       "1216   may lah|ver axz gaon / wownt yuw kahm baek tax...         come back   \n",
       "9866   ay jhahst rax|mehm|berd / ihm wer|kaxng tax|ma...  working tomorrow   \n",
       "18866  yaor ayz dax|siyv yuw / axn ax|luw|zhaxn fuwlz...    illusion fools   \n",
       "717    wer|ker biyz kaen liyv / iy|vaxn drownz kaen f...       worker bees   \n",
       "48902           muwn|rayz axn awl swuwps ahp sahm|thaxng               owl   \n",
       "35750           muwn|rayz axn awl swuwps ahp sahm|thaxng               owl   \n",
       "3741   cheynjh bax|gihnz rayt naw / leht myuw|zaxk ae...    dance commence   \n",
       "978    wihn|ter axz kah|maxng / kowld wihndz aar blow...        cold winds   \n",
       "2056   yaor kowld|naxs bernz miy / yuw staeb miy wihd...           icicles   \n",
       "\n",
       "            keyword_phonemes  gruen_score  \\\n",
       "5280             yuw aor|der     0.898392   \n",
       "1216               kahm baek     0.896483   \n",
       "9866   wer|kihng tax|maa|row     0.894500   \n",
       "18866     ax|luw|zhaxn fuwlz     0.893575   \n",
       "717             wer|ker biyz     0.893551   \n",
       "48902                    awl     0.893530   \n",
       "35750                    awl     0.893530   \n",
       "3741         daens kax|mehns     0.893291   \n",
       "978             kowld wihndz     0.891717   \n",
       "2056            ay|sax|kaxlz     0.891561   \n",
       "\n",
       "                                    text_punc  \n",
       "5280                                     None  \n",
       "1216                                     None  \n",
       "9866                                     None  \n",
       "18866                                    None  \n",
       "717                                      None  \n",
       "48902  Moonrise. An owl swoops up. Something.  \n",
       "35750  Moonrise. An owl swoops up. Something.  \n",
       "3741                                     None  \n",
       "978                                      None  \n",
       "2056                                     None  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted_gruen = df.sort_values(by='gruen_score', ascending=False)\n",
    "df_sorted_gruen.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>haiku</th>\n",
       "      <th>text_phonemes</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keyword_phonemes</th>\n",
       "      <th>gruen_score</th>\n",
       "      <th>text_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Delicate savage. / You'll never hold the cinde...</td>\n",
       "      <td>deh|lax|kaxt sae|vaxjh / yuwl neh|ver hhowld d...</td>\n",
       "      <td>cinder</td>\n",
       "      <td>sihn|der</td>\n",
       "      <td>0.639071</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>A splash and a cry. / Words pulled from the ri...</td>\n",
       "      <td>ax splaesh aend ax kray / werdz puhld frahm dh...</td>\n",
       "      <td>the riverside</td>\n",
       "      <td>dhax rih|ver|sayd</td>\n",
       "      <td>0.563353</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Steamy, mist rising. / Rocks receiving downwar...</td>\n",
       "      <td>stiy|miy mihst ray|zaxng / raaks rax|siy|vaxng...</td>\n",
       "      <td>mist rising</td>\n",
       "      <td>mihst ray|zaxng</td>\n",
       "      <td>0.538326</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>You were broken glass. / But I touched you eve...</td>\n",
       "      <td>yuw wer brow|kaxn glaes / baht ay tahcht yuw i...</td>\n",
       "      <td>broken glass</td>\n",
       "      <td>brow|kaxn glaes</td>\n",
       "      <td>0.703446</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Eyes dance with firelight. / The Moon and I ar...</td>\n",
       "      <td>ayz daens wihdh faxr|layt / dhax muwn aend ay ...</td>\n",
       "      <td>eyes dance</td>\n",
       "      <td>ayz daens</td>\n",
       "      <td>0.830985</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source                                              haiku  \\\n",
       "0  bfbarry  Delicate savage. / You'll never hold the cinde...   \n",
       "1  bfbarry  A splash and a cry. / Words pulled from the ri...   \n",
       "2  bfbarry  Steamy, mist rising. / Rocks receiving downwar...   \n",
       "3  bfbarry  You were broken glass. / But I touched you eve...   \n",
       "4  bfbarry  Eyes dance with firelight. / The Moon and I ar...   \n",
       "\n",
       "                                       text_phonemes       keywords  \\\n",
       "0  deh|lax|kaxt sae|vaxjh / yuwl neh|ver hhowld d...         cinder   \n",
       "1  ax splaesh aend ax kray / werdz puhld frahm dh...  the riverside   \n",
       "2  stiy|miy mihst ray|zaxng / raaks rax|siy|vaxng...    mist rising   \n",
       "3  yuw wer brow|kaxn glaes / baht ay tahcht yuw i...   broken glass   \n",
       "4  ayz daens wihdh faxr|layt / dhax muwn aend ay ...     eyes dance   \n",
       "\n",
       "    keyword_phonemes  gruen_score text_punc  \n",
       "0           sihn|der     0.639071      None  \n",
       "1  dhax rih|ver|sayd     0.563353      None  \n",
       "2    mihst ray|zaxng     0.538326      None  \n",
       "3    brow|kaxn glaes     0.703446      None  \n",
       "4          ayz daens     0.830985      None  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49024, 7)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.sample(n = int(0.2 * len(df)))\n",
    "df_train = df.loc[~df.index.isin(df_test.index)]\n",
    "\n",
    "#Reset the indexes\n",
    "# df_test = df_test.reset_index()\n",
    "# df_train = df_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39220, 7)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9804, 7)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Haiku(Dataset):\n",
    "#     def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "#         self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "#         self.haiku = []\n",
    "\n",
    "#         for row in df['haiku']:\n",
    "#             self.haiku.append(torch.tensor(\n",
    "#                 self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "#             ))\n",
    "                \n",
    "#         if truncate:\n",
    "#             self.haiku = self.haiku[:20000]\n",
    "            \n",
    "#         self.haiku_count = len(self.haiku)\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.haiku_count\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         return self.haiku[item]\n",
    "\n",
    "class Haiku(Dataset):  # GPT suggested\n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.haiku = []\n",
    "\n",
    "        for row in df['haiku']:\n",
    "            # Ensure line breaks are preserved\n",
    "            formatted_row = row[:-1].replace(\"/\", \"\\\\n\")\n",
    "            tokenized_haiku = self.tokenizer.encode(\n",
    "                f\"<|{control_code}|>{formatted_row}<|endoftext|>\",\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            self.haiku.append(torch.tensor(tokenized_haiku))\n",
    "                \n",
    "        if truncate:\n",
    "            self.haiku = self.haiku[:20000]\n",
    "            \n",
    "        self.haiku_count = len(self.haiku)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.haiku_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.haiku[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku = Haiku('haiku', truncate=True, gpt2_type=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=24, epochs=10, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IBDA\\.conda\\envs\\Tugas_akhir_stephen\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:31, 635.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "tensor(3.5828, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:31, 635.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "tensor(3.2607, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:31, 638.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n",
      "tensor(3.1239, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:31, 634.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4\n",
      "tensor(2.6696, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:30, 648.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 5\n",
      "tensor(2.6845, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2836it [00:04, 679.94it/s]"
     ]
    }
   ],
   "source": [
    "#Train the model on the specific data we have\n",
    "if TRAIN:\n",
    "    model = train(haiku, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model to a pkl or something so it can be reused later on\n",
    "if TRAIN:\n",
    "    torch.save(model, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prev = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "                output_list = list(generated.squeeze().numpy())\n",
    "                output_text = f\"{tokenizer.decode(output_list)}<|endoftext| >\" \n",
    "                generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|haiku|>A peashooter? \\\\n To paint the ground to see. \\\\n Look at it fresh in my heart. \\\\n Peace without pain<|endoftext|>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = generate(model.to(\"cpu\"), tokenizer, \"<|haiku|>A peashooter\", entry_count=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Volcanic ashes rise from the ground, while countless other fires will erupt from the sky.\\n\\nDuring the first stage of a fire, people will see the heavenly bodies<|endoftext| >']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model_prev.to(\"cpu\"), tokenizer, \"<|haiku|>A peashooter\", entry_count=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data, model_type):\n",
    "    generated_haiku = []\n",
    "    for i in range(len(test_data[:10])):\n",
    "        x = generate(model_type.to(\"cpu\"), tokenizer, f\"<|haiku|>{test_data[\"haiku\"].iloc[i]}\", entry_count=1)\n",
    "        generated_haiku.append(x)\n",
    "    return generated_haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "generated_haiku = text_generation(df_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Just wanna say I. / Really like you and I hope. / U feel the same way.<|endoftext|>'], ['Night fading away. / New snow in the playground. / Seats. No shade<|endoftext|>'], ['Dawn light. / The bare shadow of a tree. / Flickers on the wall.\\n\\nHappy, and a little cry<|endoftext|>'], [\"Crow's cawing. / We add a little bourbon. / To the pecan pie.<|endoftext|>\"], ['My children and I. / Race to see the sunset. / Before it fades.<|endoftext|>'], [\"Without. / A sound. / Killing frost? Um Who'll win the winter game.<|endoftext|>\"], [\"My son's birthday. / A red poinsettia leaf. / Flutters to the ground. / Everything worth talking about. / Seems to be trying to not listen<|endoftext|>\"], ['Winter silence. / Another armload. / Of junk mail. / More boobs, more arms, more dick<|endoftext|>'], ['Twitter, can you get? / Us a new president soon. / Asking for a friend.<|endoftext|>'], [\"One day they're going. / To call a holding on the. / Patriots one day. All we can do is go.<|endoftext|>\"]]\n"
     ]
    }
   ],
   "source": [
    "print(generated_haiku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Before Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_before = text_generation(df_test, model_prev)\n",
    "generated_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate After Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_after = text_generation(df_test, model)\n",
    "generated_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop to keep only generated text and add it as a new column in the dataframe\n",
    "# my_generations = []\n",
    "\n",
    "# for i in range(len(generated_haiku)):\n",
    "#     a = df_test[\"haiku\"][i].split()[-30:]  # Get the matching string we want (30 words)\n",
    "#     b = \" \".join(a)\n",
    "#     c = \" \".join(generated_haiku[i])  # Get all that comes after the matching string\n",
    "#     my_generations.append(c.split(b)[-1])\n",
    "\n",
    "# df_test[\"generated_haiku\"] = my_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finish the sentences when there is a point, remove after that\n",
    "# final = []\n",
    "\n",
    "# for i in range(len(df_test)):\n",
    "#     to_remove = df_test[\"generated_haiku\"][i].split(\".\")[-1]\n",
    "#     final.append(df_test[\"generated_haiku\"][i].replace(to_remove, \"\"))\n",
    "\n",
    "# df_test[\"generated_haiku\"] = final\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['generated_haiku'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['haiku'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRUEN for Evaluating Linguistic Quality of Generated Text\n",
    "\n",
    "**Authors:** Wanzheng Zhu, Suma Bhat\n",
    "\n",
    "**Published:** 2020\n",
    "\n",
    "**Conference:** Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\n",
    "\n",
    "**Pages:** 94-108"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tugas_akhir_stephen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
